<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3 &#8212; taku-y.github.io 0.1 documentation</title>
    
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-3.3.6/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-3.3.6/css/bootstrap-theme.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="top" title="taku-y.github.io 0.1 documentation" href="../../index.html" />
    <link rel="prev" title="ダウンロードファイル" href="../../downloads.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          taku-y.github.io</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../profile.html">プロフィール</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../git-intro.html">Git入門</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../downloads.html">ダウンロードファイル</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3</a><ul>
<li><a class="reference internal" href="#Dataset">Dataset</a></li>
<li><a class="reference internal" href="#Log-likelihood-of-documents-for-LDA">Log-likelihood of documents for LDA</a></li>
<li><a class="reference internal" href="#LDA-model">LDA model</a></li>
<li><a class="reference internal" href="#Mini-batch">Mini-batch</a></li>
<li><a class="reference internal" href="#Encoder">Encoder</a></li>
<li><a class="reference internal" href="#AEVB-with-ADVI">AEVB with ADVI</a></li>
<li><a class="reference internal" href="#Extraction-of-characteristic-words-of-topics-based-on-posterior-samples">Extraction of characteristic words of topics based on posterior samples</a></li>
<li><a class="reference internal" href="#Predictive-distribution">Predictive distribution</a></li>
<li><a class="reference internal" href="#Summary">Summary</a></li>
<li><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="../../downloads.html" title="Previous Chapter: ダウンロードファイル"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; ダウンロードファイル</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="../../_sources/notebook/20160928/lda-advi-ae.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput > div,
div.nbinput div[class^=highlight],
div.nbinput div[class^=highlight] pre,
div.nboutput,
div.nboutput > div,
div.nboutput div[class^=highlight],
div.nboutput div[class^=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class^=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput > :first-child pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput > :first-child pre {
    color: #D84315;
}

/* all prompts */
div.nbinput > :first-child[class^=highlight],
div.nboutput > :first-child[class^=highlight],
div.nboutput > :first-child {
    min-width: 9ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}

/* input/output area */
div.nbinput > :nth-child(2)[class^=highlight],
div.nboutput > :nth-child(2),
div.nboutput > :nth-child(2)[class^=highlight] {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}

/* input area */
div.nbinput > :nth-child(2)[class^=highlight] {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput  > :nth-child(2).stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="section" id="Automatic-autoencoding-variational-Bayes-for-latent-dirichlet-allocation-with-PyMC3">
<h1>Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3<a class="headerlink" href="#Automatic-autoencoding-variational-Bayes-for-latent-dirichlet-allocation-with-PyMC3" title="Permalink to this headline">¶</a></h1>
<ol class="loweralpha simple" start="3">
<li>2016 by Taku Yoshioka</li>
</ol>
<p>For probabilistic models with latent variables, autoencoding variational
Bayes (AEVB; Kingma and Welling, 2014) is an algorithm which allows us
to perform inference efficiently for large datasets with an encoder. In
AEVB, the encoder is used to infer variational parameters of approximate
posterior on latent variables from given samples. By using tunable and
flexible encoders such as multilayer perceptrons (MLPs), AEVB
approximates complex variational posterior based on mean-field
approximation, which does not utilize analytic representations of the
true posterior. Combining AEVB with ADVI (Kucukelbir et al., 2015), we
can perform posterior inference on almost arbitrary probabilistic models
involving continuous latent variables.</p>
<p>I have implemented AEVB for ADVI with mini-batch on PyMC3. To
demonstrate flexibility of this approach, we will apply this to latent
dirichlet allocation (LDA; Blei et al., 2003) for modeling documents. In
the LDA model, each document is assumed to be generated from a
multinomial distribution, whose parameters are treated as latent
variables. By using AEVB with an MLP as an encoder, we will fit the LDA
model to the 20-newsgroups dataset.</p>
<p>In this example, extracted topics by AEVB seem to be qualitatively
comparable to those with a standard LDA implementation, i.e., online VB
implemented on scikit-learn. Unfortunately, the predictive accuracy of
unseen words is less than the standard implementation of LDA, it might
be due to the mean-field approximation. However, the combination of AEVB
and ADVI allows us to quickly apply more complex probabilistic models
than LDA to big data with the help of mini-batches. I hope this notebook
will attract readers, especially practitioners working on a variety of
machine learning tasks, to probabilistic programming and PyMC3.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">os</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/git/github/taku-y/pymc3&#39;</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">shared</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">tt</span>
<span class="kn">from</span> <span class="nn">theano.sandbox.rng_mrg</span> <span class="kn">import</span> <span class="n">MRG_RandomStreams</span>

<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Dirichlet</span>
<span class="kn">from</span> <span class="nn">pymc3.distributions.transforms</span> <span class="kn">import</span> <span class="n">t_stick_breaking</span>
<span class="kn">from</span> <span class="nn">pymc3.variational.advi</span> <span class="kn">import</span> <span class="n">advi</span><span class="p">,</span> <span class="n">sample_vp</span>

<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">compute_test_value</span> <span class="o">=</span> <span class="s1">&#39;ignore&#39;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
/Users/taku-y/anaconda/anaconda/envs/py35con/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.
  &#34;`IPython.html.widgets` has moved to `ipywidgets`.&#34;, ShimWarning)
</pre></div></div>
</div>
<div class="section" id="Dataset">
<h2>Dataset<a class="headerlink" href="#Dataset" title="Permalink to this headline">¶</a></h2>
<p>Here, we will use the 20-newsgroups dataset. This dataset can be
obtained by using functions of scikit-learn. The below code is partially
adopted from an example of scikit-learn
(<a class="reference external" href="http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html">http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html</a>).
We set the number of words in the vocabulary to 1000.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># The number of words in the vocaburary</span>
<span class="n">n_words</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading dataset...&quot;</span><span class="p">)</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))</span>
<span class="n">data_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>

<span class="c1"># Use tf (raw term count) features for LDA.</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Extracting tf features for LDA...&quot;</span><span class="p">)</span>
<span class="n">tf_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">n_words</span><span class="p">,</span>
                                <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">tf</span> <span class="o">=</span> <span class="n">tf_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_samples</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">tf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Loading dataset...
done in 1.896s.
Extracting tf features for LDA...
done in 1.885s.
</pre></div></div>
</div>
<p>Each document is represented by 1000-dimensional term-frequency vector.
Let&#8217;s check the data.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../../_images/notebook_20160928_lda-advi-ae_5_0.png" src="../../_images/notebook_20160928_lda-advi-ae_5_0.png" />
</div>
</div>
<p>We split the whole documents into training and test sets. The number of
tokens in the training set is 480K. Sparsity of the term-frequency
document matrix is 0.025%, which implies almost all components in the
term-frequency matrix is zero.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">n_samples_tr</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_samples_te</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">n_samples_tr</span>
<span class="n">docs_tr</span> <span class="o">=</span> <span class="n">tf</span><span class="p">[:</span><span class="n">n_samples_tr</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">docs_te</span> <span class="o">=</span> <span class="n">tf</span><span class="p">[</span><span class="n">n_samples_tr</span><span class="p">:,</span> <span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of docs for training = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of docs for test = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">docs_te</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">n_tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">docs_tr</span><span class="p">[</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of tokens in training set = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Sparsity = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Number of docs for training = 10000
Number of docs for test = 1314
Number of tokens in training set = 480287
Sparsity = 0.0253837
</pre></div></div>
</div>
</div>
<div class="section" id="Log-likelihood-of-documents-for-LDA">
<h2>Log-likelihood of documents for LDA<a class="headerlink" href="#Log-likelihood-of-documents-for-LDA" title="Permalink to this headline">¶</a></h2>
<p>For a document <span class="math">\(d\)</span> consisting of tokens <span class="math">\(w\)</span>, the
log-likelihood of the LDA model with <span class="math">\(K\)</span> topics is given as</p>
<div class="math">
\begin{eqnarray}
    \log p\left(d|\theta_{d},\beta\right) &amp; = &amp; \sum_{w\in d}\log\left[\sum_{k=1}^{K}\exp\left(\log\theta_{d,k} + \log \beta_{k,w}\right)\right]+const,
\end{eqnarray}</div><p>where <span class="math">\(\theta_{d}\)</span> is the topic distribution for document
<span class="math">\(d\)</span> and <span class="math">\(\beta\)</span> is the word distribution for the <span class="math">\(K\)</span>
topics. We define a function that returns a tensor of the log-likelihood
of documents given <span class="math">\(\theta_{d}\)</span> and <span class="math">\(\beta\)</span>.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">logp_lda_doc</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the log-likelihood function for given documents.</span>

<span class="sd">    K : number of topics in the model</span>
<span class="sd">    V : number of words (size of vocabulary)</span>
<span class="sd">    D : number of documents (in a mini-batch)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    beta : tensor (K x V)</span>
<span class="sd">        Word distributions.</span>
<span class="sd">    theta : tensor (D x K)</span>
<span class="sd">        Topic distributions for documents.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">ll_docs_f</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
        <span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
        <span class="n">vfreqs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span><span class="p">]</span>
        <span class="n">ll_docs</span> <span class="o">=</span> <span class="n">vfreqs</span> <span class="o">*</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="n">dixs</span><span class="p">])</span> <span class="o">+</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">vixs</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="c1"># Per-word log-likelihood times num of tokens in the whole dataset</span>
        <span class="k">return</span> <span class="n">tt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ll_docs</span><span class="p">)</span> <span class="o">/</span> <span class="n">tt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vfreqs</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_tokens</span>

    <span class="k">return</span> <span class="n">ll_docs_f</span>
</pre></div>
</div>
</div>
<p>In the inner function, the log-likelihood is scaled for mini-batches by
the number of tokens in the dataset.</p>
</div>
<div class="section" id="LDA-model">
<h2>LDA model<a class="headerlink" href="#LDA-model" title="Permalink to this headline">¶</a></h2>
<p>With the log-likelihood function, we can construct the probabilistic
model for LDA. <code class="docutils literal"><span class="pre">doc_t</span></code> works as a placeholder to which documents in a
mini-batch are set.</p>
<p>For ADVI, each of random variables <span class="math">\(\theta\)</span> and <span class="math">\(\beta\)</span>,
drawn from Dirichlet distributions, is transformed into unconstrained
real coordinate space. To do this, by default, PyMC3 uses a centered
stick-breaking transformation. Since these random variables are on a
simplex, the dimension of the unconstrained coordinate space is the
original dimension minus 1. For example, the dimension of
<span class="math">\(\theta_{d}\)</span> is the number of topics (<code class="docutils literal"><span class="pre">n_topics</span></code>) in the LDA
model, thus the transformed space has dimension <code class="docutils literal"><span class="pre">(n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code>. It
shuold be noted that, in this example, we use <code class="docutils literal"><span class="pre">t_stick_breaking</span></code>,
which is a numerically stable version of <code class="docutils literal"><span class="pre">stick_breaking</span></code> used by
default. This is required to work ADVI for the LDA model.</p>
<p>The variational posterior on these transformed parameters is represented
by a spherical Gaussian distributions (meanfield approximation). Thus,
the number of variational parameters of <span class="math">\(\theta_{d}\)</span>, the latent
variable for each document, is <code class="docutils literal"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">(n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code> for means and
standard deviations.</p>
<p>In the last line of the below cell, <code class="docutils literal"><span class="pre">DensityDist</span></code> class is used to
define the log-likelihood function of the model. The second argument is
a Python function which takes observations (a document matrix in this
example) and returns the log-likelihood value. This function is given as
a return value of <code class="docutils literal"><span class="pre">logp_lda_doc(beta,</span> <span class="pre">theta)</span></code>, which has been defined
above.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Tensor for documents</span>
<span class="n">doc_t</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_words</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;doc_t&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
                     <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">))</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;doc&#39;</span><span class="p">,</span> <span class="n">logp_lda_doc</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">doc_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Applied stickbreaking-transform to theta and added transformed theta_stickbreaking_ to model.
Applied stickbreaking-transform to beta and added transformed beta_stickbreaking_ to model.
</pre></div></div>
</div>
</div>
<div class="section" id="Mini-batch">
<h2>Mini-batch<a class="headerlink" href="#Mini-batch" title="Permalink to this headline">¶</a></h2>
<p>To perform ADVI with stochastic variational inference for large
datasets, whole training samples are splitted into mini-batches. PyMC3&#8217;s
ADVI function accepts a Python generator which send a list of
mini-batches to the algorithm. Here is an example to make a generator.</p>
<p>TODO: replace the code using the new interface</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">create_minibatch</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># Return random data samples of a size &#39;minibatch_size&#39; at each iteration</span>
        <span class="n">ixs</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">minibatch_size</span><span class="p">)</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="n">ixs</span><span class="p">]]</span>

<span class="n">minibatches</span> <span class="o">=</span> <span class="n">create_minibatch</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The ADVI function replaces the values of Theano tensors with samples
given by generators. We need to specify those tensors by a list. The
order of the list should be the same with the mini-batches sent from the
generator. Note that <code class="docutils literal"><span class="pre">doc_t</span></code> has been used in the model creation as
the observation of the random variable named <code class="docutils literal"><span class="pre">doc</span></code>.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># The value of doc_t will be replaced with mini-batches</span>
<span class="n">minibatch_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc_t</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>To tell the algorithm that random variable <code class="docutils literal"><span class="pre">doc</span></code> is observed, we need
to pass them as an <code class="docutils literal"><span class="pre">OrderedDict</span></code>. The key of <code class="docutils literal"><span class="pre">OrderedDict</span></code> is an
observed random variable and the value is a scalar representing the
scaling factor. Since the likelihood of the documents in mini-batches
have been already scaled in the likelihood function, we set the scaling
factor to 1.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># observed_RVs = OrderedDict([(doc, n_samples_tr / minibatch_size)])</span>
<span class="n">observed_RVs</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">doc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Encoder">
<h2>Encoder<a class="headerlink" href="#Encoder" title="Permalink to this headline">¶</a></h2>
<p>Given a document, the encoder calculates variational parameters of the
(transformed) latent variables, more specifically, parameters of
Gaussian distributions in the unconstrained real coordinate space. The
<code class="docutils literal"><span class="pre">encode()</span></code> method is required to output variational means and stds as
a tuple, as shown in the following code. As explained above, the number
of variational parameters is <code class="docutils literal"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">(n_topics)</span> <span class="pre">-</span> <span class="pre">1</span></code>. Specifically, the
shape of <code class="docutils literal"><span class="pre">zs_mean</span></code> (or <code class="docutils literal"><span class="pre">zs_std</span></code>) in the method is
<code class="docutils literal"><span class="pre">(minibatch_size,</span> <span class="pre">n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code>. It should be noted that <code class="docutils literal"><span class="pre">zs_std</span></code>
is defined as log-transformed standard deviation and this is
automativally exponentiated (thus bounded to be positive) in
<code class="docutils literal"><span class="pre">advi_minibatch()</span></code>, the estimation function.</p>
<p>To enhance generalization ability to unseen words, a bernoulli
corruption process is applied to the inputted documents. Unfortunately,
I have never see any significant improvement with this.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">LDAEncoder</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Encode (term-frequency) document vectors to variational means and (log-transformed) stds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">,</span> <span class="n">p_corruption</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">=</span> <span class="n">n_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">=</span> <span class="n">n_topics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w0</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w0&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b0</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b0&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">MRG_RandomStreams</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span> <span class="o">=</span> <span class="n">p_corruption</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span><span class="p">:</span>
            <span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">set_subtensor</span><span class="p">(</span>
                <span class="n">tt</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">xs</span><span class="p">)[</span><span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">dixs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">xs_</span> <span class="o">=</span> <span class="n">xs</span> <span class="o">*</span> <span class="n">mask</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xs_</span> <span class="o">=</span> <span class="n">xs</span>

        <span class="n">w0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w0</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">))</span>
        <span class="n">w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">hs</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">xs_</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w0</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">)</span>
        <span class="n">zs</span> <span class="o">=</span> <span class="n">hs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
        <span class="n">zs_mean</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[:,</span> <span class="p">:(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">zs_std</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[:,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):]</span>
        <span class="k">return</span> <span class="n">zs_mean</span><span class="p">,</span> <span class="n">zs_std</span>

    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">w0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>To feed the output of the encoder to the variational parameters of
<span class="math">\(\theta\)</span>, we set an OrderedDict of tuples as below.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">LDAEncoder</span><span class="p">(</span><span class="n">n_words</span><span class="o">=</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_topics</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">p_corruption</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">local_RVs</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">theta</span><span class="p">,</span> <span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">doc_t</span><span class="p">),</span> <span class="n">n_samples_tr</span> <span class="o">/</span> <span class="n">minibatch_size</span><span class="p">))])</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">theta</span></code> is the random variable defined in the model creation and is a
key of an entry of the <code class="docutils literal"><span class="pre">OrderedDict</span></code>. The value
<code class="docutils literal"><span class="pre">(encoder.encode(doc_t),</span> <span class="pre">n_samples_tr</span> <span class="pre">/</span> <span class="pre">minibatch_size)</span></code> is a tuple of
a theano expression and a scalar. The theano expression
<code class="docutils literal"><span class="pre">encoder.encode(doc_t)</span></code> is the output of the encoder given inputs
(documents). The scalar <code class="docutils literal"><span class="pre">n_samples_tr</span> <span class="pre">/</span> <span class="pre">minibatch_size</span></code> specifies the
scaling factor for mini-batches.</p>
<p>ADVI optimizes the parameters of the encoder. They are passed to the
function for ADVI.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">encoder_params</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="AEVB-with-ADVI">
<h2>AEVB with ADVI<a class="headerlink" href="#AEVB-with-ADVI" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">advi_minibatch()</span></code> can be used to run AEVB with ADVI on the LDA model.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">run_advi</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">v_params</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">variational</span><span class="o">.</span><span class="n">advi_minibatch</span><span class="p">(</span>
            <span class="n">n</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">minibatch_tensors</span><span class="o">=</span><span class="n">minibatch_tensors</span><span class="p">,</span> <span class="n">minibatches</span><span class="o">=</span><span class="n">minibatches</span><span class="p">,</span>
            <span class="n">local_RVs</span><span class="o">=</span><span class="n">local_RVs</span><span class="p">,</span> <span class="n">observed_RVs</span><span class="o">=</span><span class="n">observed_RVs</span><span class="p">,</span> <span class="n">encoder_params</span><span class="o">=</span><span class="n">encoder_params</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-2</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_mcsamples</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">v_params</span>

<span class="o">%</span><span class="k">time</span> v_params = run_advi()
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">v_params</span><span class="o">.</span><span class="n">elbo_vals</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Iteration 0 [0%]: ELBO = -3722434.15
Iteration 300 [10%]: Average ELBO = -3541472.9
Iteration 600 [20%]: Average ELBO = -3448505.31
Iteration 900 [30%]: Average ELBO = -3384165.84
Iteration 1200 [40%]: Average ELBO = -3327991.5
Iteration 1500 [50%]: Average ELBO = -3287262.03
Iteration 1800 [60%]: Average ELBO = -3270108.58
Iteration 2100 [70%]: Average ELBO = -3258014.51
Iteration 2400 [80%]: Average ELBO = -3246227.58
Iteration 2700 [90%]: Average ELBO = -3238394.33
Finished [100%]: ELBO = -3242038.97
CPU times: user 43.6 s, sys: 866 ms, total: 44.5 s
Wall time: 39.7 s
</pre></div></div>
</div>
<div class="nboutput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[13]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>[&lt;matplotlib.lines.Line2D at 0x115135d68&gt;]
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../../_images/notebook_20160928_lda-advi-ae_26_2.png" src="../../_images/notebook_20160928_lda-advi-ae_26_2.png" />
</div>
</div>
<p>We can see ELBO increases as optimization proceeds. The trace of ELBO
looks jaggy because at each iteration documents in the mini-batch are
replaced.</p>
</div>
<div class="section" id="Extraction-of-characteristic-words-of-topics-based-on-posterior-samples">
<h2>Extraction of characteristic words of topics based on posterior samples<a class="headerlink" href="#Extraction-of-characteristic-words-of-topics-based-on-posterior-samples" title="Permalink to this headline">¶</a></h2>
<p>By using estimated variational parameters, we can draw samples from the
variational posterior. To do this, we use function <code class="docutils literal"><span class="pre">sample_vp()</span></code>. Here
we use this function to obtain posterior mean of the word-topic
distribution <span class="math">\(\beta\)</span> and show top-10 words frequently appeared in
the 10 topics.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">print_top_words</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_words</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beta</span><span class="p">)):</span>
        <span class="k">print</span><span class="p">((</span><span class="s2">&quot;Topic #</span><span class="si">%d</span><span class="s2">: &quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">feature_names</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="o">-</span><span class="n">n_top_words</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>

<span class="n">doc_t</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">docs_te</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)[:</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="p">:])</span>

<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">sample_vp</span><span class="p">(</span><span class="n">v_params</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">local_RVs</span><span class="o">=</span><span class="n">local_RVs</span><span class="p">)</span>
    <span class="n">beta_pymc3</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">print_top_words</span><span class="p">(</span><span class="n">beta_pymc3</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Topic #0: don think just know make going like people want sure
Topic #1: year team game play win games players season period new
Topic #2: edu information com mail list send available university 1993 email
Topic #3: people state government gun world said years war states armenian
Topic #4: god people believe does true jesus say question life way
Topic #5: windows use thanks drive using window card file does work
Topic #6: key use chip encryption government public keys used law clipper
Topic #7: did time didn got said just day right thought let
Topic #8: good like ve better really car probably lot know problem
Topic #9: new power space 10 00 years used price 50 high
</pre></div></div>
</div>
<p>We compare these topics to those obtained by a standard LDA
implementation on scikit-learn, which is based on an online stochastic
variational inference (Hoffman et al., 2013). We can see that estimated
words in the topics are qualitatively similar.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_topics</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                <span class="n">learning_method</span><span class="o">=</span><span class="s1">&#39;online&#39;</span><span class="p">,</span> <span class="n">learning_offset</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> lda.fit(docs_tr)
<span class="n">beta_sklearn</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">components_</span> <span class="o">/</span> <span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">print_top_words</span><span class="p">(</span><span class="n">beta_sklearn</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
CPU times: user 11.8 s, sys: 11.7 ms, total: 11.8 s
Wall time: 11.8 s
Topic #0: government people law mr president gun state states public rights
Topic #1: drive card scsi bit disk use mac hard memory does
Topic #2: people armenian said armenians turkish did war killed saw russian
Topic #3: year just good time team game car years like think
Topic #4: 10 00 25 15 20 12 11 14 16 17
Topic #5: windows window program dos file use version display ms application
Topic #6: edu space com file information mail data available send ftp
Topic #7: ax max g9v pl b8f a86 34u 145 1t 75u
Topic #8: god people jesus believe does think say life don true
Topic #9: don like know just think ve does use want good
</pre></div></div>
</div>
</div>
<div class="section" id="Predictive-distribution">
<h2>Predictive distribution<a class="headerlink" href="#Predictive-distribution" title="Permalink to this headline">¶</a></h2>
<p>In some papers (e.g., Hoffman et al. 2013), the predictive distribution
of held-out words was proposed as a quantitative measure for goodness of
the model fitness. The log-likelihood function for tokens of the
held-out word can be calculated with posterior means of <span class="math">\(\theta\)</span>
and <span class="math">\(\beta\)</span>. The validity of this is explained in (Hoffman et al.
2013).</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">calc_pp</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">wix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    ws: ndarray (N,)</span>
<span class="sd">        Number of times the held-out word appeared in N documents.</span>
<span class="sd">    thetas: ndarray, shape=(N, K)</span>
<span class="sd">        Topic distributions for N documents.</span>
<span class="sd">    beta: ndarray, shape=(K, V)</span>
<span class="sd">        Word distributions for K topics.</span>
<span class="sd">    wix: int</span>
<span class="sd">        Index of the held-out word</span>

<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    Log probability of held-out words.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ws</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">thetas</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">eval_lda</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">docs_te</span><span class="p">,</span> <span class="n">wixs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate LDA model by log predictive probability.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    transform: Python function</span>
<span class="sd">        Transform document vectors to posterior mean of topic proportions.</span>
<span class="sd">    wixs: iterable of int</span>
<span class="sd">        Word indices to be held-out.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lpss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">docs_</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">docs_te</span><span class="p">)</span>
    <span class="n">thetass</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">wss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_words</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">wix</span> <span class="ow">in</span> <span class="n">wixs</span><span class="p">:</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">docs_te</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">():</span>
            <span class="c1"># Hold-out</span>
            <span class="n">docs_</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Topic distributions</span>
            <span class="n">thetas</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">docs_</span><span class="p">)</span>

            <span class="c1"># Predictive log probability</span>
            <span class="n">lpss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_pp</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">wix</span><span class="p">))</span>

            <span class="n">docs_</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span> <span class="o">=</span> <span class="n">ws</span>
            <span class="n">thetass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>
            <span class="n">wss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ws</span><span class="p">)</span>
            <span class="n">total_words</span> <span class="o">+=</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">thetass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
            <span class="n">wss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>

    <span class="c1"># Log-probability</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">lpss</span><span class="p">))</span> <span class="o">/</span> <span class="n">total_words</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lp&#39;</span><span class="p">:</span> <span class="n">lp</span><span class="p">,</span>
        <span class="s1">&#39;thetass&#39;</span><span class="p">:</span> <span class="n">thetass</span><span class="p">,</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;wss&#39;</span><span class="p">:</span> <span class="n">wss</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
<p>To apply the above function for the LDA model, we redefine the
probabilistic model because the number of documents to be tested
changes. Since variational parameters have already been obtained, we can
reuse them for sampling from the approximate posterior distribution.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">n_docs_te</span> <span class="o">=</span> <span class="n">docs_te</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">doc_t</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="n">docs_te</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;doc_t&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_docs_te</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_docs_te</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">),</span>
                     <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">))</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;doc&#39;</span><span class="p">,</span> <span class="n">logp_lda_doc</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">doc_t</span><span class="p">)</span>

<span class="c1"># Encoder has already been trained</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">p_corruption</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">local_RVs</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">theta</span><span class="p">,</span> <span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">doc_t</span><span class="p">),</span> <span class="mi">1</span><span class="p">))])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Applied stickbreaking-transform to theta and added transformed theta_stickbreaking_ to model.
Applied stickbreaking-transform to beta and added transformed beta_stickbreaking_ to model.
</pre></div></div>
</div>
<p><code class="docutils literal"><span class="pre">transform()</span></code> function is defined with <code class="docutils literal"><span class="pre">sample_vp()</span></code> function. This
function is an argument to the function for calculating log predictive
probabilities.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">transform_pymc3</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">doc_t</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sample_vp</span><span class="p">(</span><span class="n">v_params</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">local_RVs</span><span class="o">=</span><span class="n">local_RVs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The mean of the log predictive probability is -7.00.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">time</span> result_pymc3 = eval_lda(transform_pymc3, beta_pymc3, docs_te.toarray().astype(&#39;float32&#39;), np.arange(100))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Predictive log prob (pm3) = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result_pymc3</span><span class="p">[</span><span class="s1">&#39;lp&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
CPU times: user 4min 1s, sys: 3.76 s, total: 4min 4s
Wall time: 1min 45s
Predictive log prob (pm3) = -7.104652449902257
</pre></div></div>
</div>
<p>We compare the result with the scikit-learn LDA implemented The log
predictive probability is significantly higher (-6.04) than AEVB-ADVI,
though it shows similar words in the estimated topics. It may because
that the mean-field approximation to distribution on the simplex (topic
and/or word distributions) is less accurate. See
<a class="reference external" href="https://gist.github.com/taku-y/f724392bc0ad633deac45ffa135414d3">https://gist.github.com/taku-y/f724392bc0ad633deac45ffa135414d3</a>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">transform_sklearn</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">thetas</span> <span class="o">/</span> <span class="n">thetas</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="o">%</span><span class="k">time</span> result_sklearn = eval_lda(transform_sklearn, beta_sklearn, docs_te.toarray(), np.arange(100))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Predictive log prob (sklearn) = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result_sklearn</span><span class="p">[</span><span class="s1">&#39;lp&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
CPU times: user 25.7 s, sys: 41.8 ms, total: 25.7 s
Wall time: 25.7 s
Predictive log prob (sklearn) = -6.041000209921997
</pre></div></div>
</div>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h2>
<p>We have seen that PyMC3 allows us to estimate random variables of LDA, a
probabilistic model with latent variables, based on automatic
variational inference. Variational parameters of the local latent
variables in the probabilistic model are encoded from observations. The
parameters of the encoding model, MLP in this example, are optimized
with variational parameters of the global latent variables. Once the
probabilistic and the encoding models are defined, parameter
optimization is done just by invoking a function (<code class="docutils literal"><span class="pre">advi_minibatch()</span></code>)
without need to derive complex update equations.</p>
<p>Unfortunately, the estimation result was not accurate compared to LDA in
sklearn, which is based on the conjugate priors and thus not relying on
the mean field approximation. To improve the estimation accuracy, some
researchers proposed post processings that moves Monte Carlo samples to
improve variational lower bound (e.g., Rezende and Mohamed, 2015;
Salinams et al., 2015). By implementing such methods on PyMC3, we may
achieve more accurate estimation while automated as shown in this
notebook.</p>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes.
stat, 1050, 1.</li>
<li>Kucukelbir, A., Ranganath, R., Gelman, A., &amp; Blei, D. (2015).
Automatic variational inference in Stan. In Advances in neural
information processing systems (pp. 568-576).</li>
<li>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet
allocation. Journal of machine Learning research, 3(Jan), 993-1022.</li>
<li>Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. W. (2013).
Stochastic variational inference. Journal of Machine Learning
Research, 14(1), 1303-1347.</li>
<li>Rezende, D. J., &amp; Mohamed, S. (2015). Variational inference with
normalizing flows. arXiv preprint arXiv:1505.05770.</li>
<li>Salimans, T., Kingma, D. P., &amp; Welling, M. (2015). Markov chain Monte
Carlo and variational inference: Bridging the gap. In International
Conference on Machine Learning (pp. 1218-1226).</li>
</ul>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016, Taku Yoshioka.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.4.8.<br/>
    </p>
  </div>
</footer>
  </body>
</html>